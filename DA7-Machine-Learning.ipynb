{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 222](https://github.com/GonzagaCPSC222) Intro to Data Science\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "## DA7 Machine Learning (100 pts)\n",
    "\n",
    "## Learner Objectives\n",
    "At the conclusion of this programming assignment, participants should be able to:\n",
    "* Trace the kNN algorithm on a dataset\n",
    "* Utilize a kNN classifier and a decision tree classifier from Sci-kit Learn\n",
    "* Utilize a linear regressor and a decision tree regressor from Sci-kit Learn\n",
    "* Evaluate models using train/test sets sampled using the hold out method\n",
    " \n",
    "## Prerequisites\n",
    "Before starting this programming assignment, participants should be able to:\n",
    "* Use SciPy to perform hypothesis testing using t-tests\n",
    "* Use Jupyter Notebook to tell a data science story\n",
    "* Use Pandas for data analysis\n",
    "* Use Matplotlib to visualize data\n",
    "\n",
    "## Acknowledgments\n",
    "Content used in this assignment is based upon information in the following sources:\n",
    "* Kaggle's [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/)\n",
    "* [Google Trends](https://trends.google.com/trends/)\n",
    "* Sci-kit Learn's [Boston house prices dataset](https://scikit-learn.org/stable/datasets/index.html#boston-house-prices-dataset)\n",
    "* [Make School](https://www.makeschool.com/)'s decision trees and regression techniques assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github Classroom Setup\n",
    "For this assignment, you will use GitHub Classroom to create a private code repositories to track code changes and submit your assignment. Open this DA7 link to accept the assignment and create a private repository for your assignment in Github classroom: https://classroom.github.com/a/OB_LIcGf\n",
    "\n",
    "Your repo, for example, will be named GonzagaCPSC222/da7-yourusername (where yourusername is your Github username). I highly recommend committing/pushing regularly so your work is always backed up. We will grade your most recent commit, even if that commit is after the due date (your work will be marked late if this is the case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Tracing Exercise (10 pts)\n",
    "In a Jupyter Notebook called kNNTrace.ipynb, trace the kNN algorithm on the following labeled dataset, where \"Family Car\" denotes class information and the remaining columns have continuous values:\n",
    "\n",
    "Price|Engine Power|Family Car\n",
    "-|-|-\n",
    "7000|310|no\n",
    "8000|180|no\n",
    "14000|200|no\n",
    "15000|280|yes\n",
    "20000|250|yes\n",
    "20000|340|no\n",
    "22000|300|yes\n",
    "25000|260|yes\n",
    "27000|285|yes\n",
    "29000|340|no\n",
    "30000|210|no\n",
    "39000|160|no\n",
    "40000|245|no\n",
    "41000|285|no\n",
    "\n",
    "Assume we have the following instance to classify using kNN with k = 3:\n",
    "\n",
    "Price|Engine Power|Family Car\n",
    "-|-|-\n",
    "21000|190|?\n",
    "\n",
    "How would you classify the test instance? Show your work (e.g. do not use Sci-kit Learn to do this) for finding the three closest neighbors of the above test instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit Learn Titanic Classification (35 pts)\n",
    "In a Jupyter Notebook called TitanicClassification.ipynb, we are going to tell a story about exploratory data analysis (EDA) and classification of a \"classic\" machine learning dataset, the [Titanic survival dataset](https://www.kaggle.com/c/titanic). Here is a description of the features and the class (\"Survived\"):\n",
    "\n",
    "Variable|Definition|Key\n",
    "-|-|-\n",
    "Survived|Survival|0 = No, 1 = Yes\n",
    "Pclass|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "Sex|Sex|\n",
    "Age|Age in years|\n",
    "SibSp|# of siblings / spouses aboard the Titanic|\n",
    "Parch|# of parents / children aboard the Titanic|\n",
    "Ticket|Ticket number|\n",
    "Fare|Passenger fare|\n",
    "Cabin|Cabin number|\n",
    "Embarked|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "\n",
    "### Collect/Clean/Prepare Data\n",
    "1. Download the titanic.csv file from the DAs repo on Github: https://github.com/GonzagaCPSC222/DAs/blob/master/files.\n",
    "1. Load the data, setting the \"PassengerId\" column to be the DataFrame index\n",
    "1. Clean the data\n",
    "    1. Drop columns that are not informative (e.g. at least ones that are unique to each passenger)\n",
    "    1. Convert remaining categorical feature(s) to be numeric. Use [sklearn.preprocessing.LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to do this\n",
    "    1. Drop rows that have a missing value\n",
    "    \n",
    "### Exploratory Data Analysis\n",
    "If you've seen the [Titanic movie](https://en.wikipedia.org/wiki/Titanic_(1997_film)), you likely already have a few hunches (or more scientifically, hypotheses) about which of these features are predictive of survival. If you haven't seen the movie, no worries! EDA will help you develop a few hypotheses of your own.\n",
    "1. Group the data by \"Survived\" and use [`get_group()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.get_group.html) and [`describe()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) to provide some summary stats/discussion points for those that survived and didn't survive.\n",
    "1. Create at least two visualizations of the data to explore what features are or are not related to survivability.\n",
    "1. Apply t-tests to test at least two of your hypotheses about which features can be used to discriminate between those who survived and didn't survive.\n",
    "1. Store the \"Survived\" class labels in a separate 1D data structure (e.g. separate the y from the X)\n",
    "1. Train a [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) on the entire dataset. Set the `max_depth` of the tree to be 3 so your decision tree isn't really tall. \n",
    "    1. Create a visualization of the tree using [sklearn.tree.plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html). Set the appropriate keyword arguments so you can look at your tree and easily interpret the rules. For example, `plot_tree(clf, feature_names=X.columns, class_names={1: \"survived\", 0: \"died\"}, filled=True)`\n",
    "    1. Decision trees are great for EDA because they naturally do a process called \"feature selection.\" That is, the more informative/discriminatory features will be towards the top of the tree. How do your hypotheses compare to your tree output?\n",
    "    \n",
    "### Build/Refine Machine Learning Models\n",
    "1. Scale the features using [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)\n",
    "1. Evaluate the accuracy of a [sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) classifier using the hold out method for dividing the dataset in to training and testing sets\n",
    "    1. Use a `random_state` of 0 for reproducibility\n",
    "    1. Reserve 25% of the dataset for testing\n",
    "    1. Explore improving the clasifier by trying different values for the number of neighbors k. What is the best accuracy you can achieve?\n",
    "1. Using the same train/test splits, compare your KNeighborsClassifier results to results from a [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus (10 pts)\n",
    "Participate in the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) Kaggle Competition. Follow the directions and submit your predictions for their provided test set using your \"best\" classifier from the Titanic exercises above. Note that your actual accuracy for this isn't what is important. What is important is just trying it out and having fun with it :)\n",
    "\n",
    "Include your code to produce the predictions, the CSV file your uploaded for your predictions, and a screenshot of submission confirmation (with the accuracy shown) on Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Trends Regression Exercise (10 pts)\n",
    "In a Jupyter Notebook called GoogleTrendsRegression.ipynb, we are going to use the least squares linear regression algorithm to fit a line. For the dataset, we are going to download data from Google Trends. From [Wikipedia](https://en.wikipedia.org/wiki/Google_Trends):\n",
    ">Google Trends is a website by Google that analyzes the popularity of top search queries in Google Search across various regions and languages. The website uses graphs to compare the search volume of different queries over time.\n",
    "\n",
    "Go to https://trends.google.com/trends and play around a bit (there are some fun starter trends on the homepage):\n",
    "\n",
    "<img src=\"https://github.com/GonzagaCPSC222/DAs/raw/master/figures/google_trends.png\" width=\"400\">\n",
    "\n",
    "After you have a sense for how Google Trends works, explore search terms/topics that you think are correlated with \"Gonzaga University.\" Make sure these search terms/topics do not have \"Gonzaga\" or \"University\" in them. \n",
    "\n",
    "Download the \"Interest over time\" data for the past 5 years for one of your search terms/topics compared with \"Gonzaga University.\" Perform data cleaning and preparation on this dataset so you can fit a line to it. Our \"x\" values will be the weekly search popularity for your search term/topic and our \"y\" values will be the corresponding weekly search popularity for \"Gonzaga University.\" Fit a line y = mx + b to the data by calculating m and b (do not use Sci-kit Learn to do this). Print the resulting linear equation and its correlation coefficient. Display a scatter chart of the x,y data with the linear regression line. How well does your search/term topic predict \"Gonzaga University\" popularity? I challenge you to find a search/term topic with a correlation to \"Gonzaga University\" of at least 0.5 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit Learn Housing Regression (20 pts)\n",
    "In a Jupyter Notebook called HousingRegression.ipynb, we are going to tell a story about exploratory data analysis (EDA) and regression of a \"classic\" machine learning dataset, the [Boston house prices dataset](https://scikit-learn.org/stable/datasets/index.html#boston-house-prices-dataset). \n",
    "\n",
    "### Collect/Clean/Prepare Data\n",
    "1. Load this dataset straight from the Sci-kit Learn library using the code below.\n",
    "1. Clean the data\n",
    "    1. Run `print(boston.DESCR)` to see a description of the dataset. You'll the feature, \"B\", is a problematic feature to use when building a model (recall our readings from \"Weapons of Math Destruction!!). Remove this features from the dataset.\n",
    "    1. Drop rows that have a missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the load_boston() function from sklearn.datasets\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "boston = load_boston()\n",
    "print(boston.DESCR)\n",
    "\n",
    "# create a dataframe object out of the data.  \n",
    "# Use boston.data for the data, and pass boston.feature_names \n",
    "# for the columns parameter\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "# the MEDV data in boston.target is the y variable\n",
    "df[\"MEDV\"] = boston.target # MEDV, median value of home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "For EDA for this regression problem, let's produce a correlation matrix heatmap using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df.corr()\n",
    "corr_df.style.background_gradient(cmap='bwr').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features are highly correlated with our target variable? Which features are not correlated with our target variable? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build/Refine Machine Learning Models\n",
    "1. Evaluate the coefficient of determination $R^2$ of a [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) regressor using the hold-out method for dividing the dataset in to training and testing sets\n",
    "    1. Use a `random_state` of 0 for reproducibility\n",
    "    1. Reserve 25% of the dataset for testing\n",
    "    1. Plot the predicted values against the actual values in a scatter chart\n",
    "    1. Explore improving the regressor by removing features based on their correlation with the target variable. What is the best $R^2$ you can achieve?\n",
    "1. Using the same train/test splits, compare your LinearRegression results to results from a [sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ethics (15 pts)\n",
    "Read one chapter and the conclusion from [Weapons of Math Destruction](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815) by Cathy O'Neill. This book is a NY Times bestseller, National Book Award longlist winner, and frequently mentioned as one of the top non-technical data science/big data books that everyone should read (here are a few lists: [kdnuggets](https://www.kdnuggets.com/2019/12/non-technical-reading-list-data-science.html), [dataquest](https://www.dataquest.io/blog/data-science-books/), [builtin](https://builtin.com/data-science/data-science-books), etc.). Here are the chapters to choose from:\n",
    "* Chapter 3 Arms Race: Going to College\n",
    "* Chapter 4 Propaganda Machine: Online Advertising\n",
    "* Chapter 5 Civilian Casualities: Justice in the Age of Big Data\n",
    "\n",
    "You don't need to purchase this book, unless you want a hard-copy or you want to read the whole thing. I'll [post the sections we will read to Piazza](https://piazza.com/class/ka4awqqd2opro?cid=15) so they are not publicly available on Github.\n",
    "\n",
    "In a Jupyter Notebook called Ethics.ipynb, provide your reflection on the following discussion points:\n",
    "1. Provide a summary of the chapter you chose. Have you had (or heard of) experiences similar to O'Neill's examples in the chapter? What do you think can/should be done to prevent WMDs from causing harm in this particular domain?\n",
    "1. What else struck you about this chapter?\n",
    "1. In the conclusion, O'Neill states that the first step to regulating mathematical models is to begin with the modelers themselves. That's you! In the above exercises you constructed models and used them to make predictions. While models built on the Titanic dataset seem inoccuous, we have to remember that each instance in the dataset represents a *person* and a feature like *Pclass* encodes a peron's socio-economic status. You're also building models that represent yourself for the project. That all being said, do you pledge to the Hippocratic Oath? Like O'Neill, do you have additional terms you think should be added to the Oath?\n",
    "1. O'Neill points out that the \"Hippocratic Oath ignores the on-the-ground pressure that data scientists often confront when bosses pushes for specific answers.\" Even if you don't become a \"data scientist\", you are going to be working with data in some form (e.g. careers in finance, marketing, social sciences, etc. all routinely work with data). Consequently, it is very likely that you'll be in a similar situation to what O'Neill describes at some point (sooner than later) in your career. Do you feel your experiences before Gonzaga, at Gonzaga, in this class, and from reading O'Neill's book provided have equipped you with the knowledge and skills to properly handle this situation? In closing, always remember that an \"instance\" in a \"dataset\" is often sampled from a human being, just like you. We are not equal to our data trails.\n",
    "\n",
    "This write-up should be written using full sentences and should be grammaticallly correct. Proof read your writing before you submit it!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Assignments\n",
    "1. Use Github classroom to submit your assignment via a Github repo. See the \"Github Classroom Setup\" section at the beginning of this document for details on how to do this. You must commit your solution by the due date and time.\n",
    "1. Your repo should contain only your .ipynb file(s) and your .csv and .json file(s). Double check that this is the case by cloning (or downloading a zip) your submission repo and running your code from Jupyter Lab like we will when we grade your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Guidelines\n",
    "This assignment is worth 100 points + 10 points bonus. Your assignment will be evaluated based on a successful execution in Jupyter Lab (using the Anaconda Python Distribution v3.8) and adherence to the program requirements. We will grade according to the following criteria:\n",
    "* 10 pts for tracing kNN on the family car dataset\n",
    "* 35 pts for Titanic classification\n",
    "    * 5 pts for cleaning the data\n",
    "    * 5 pts for at least two visualizations\n",
    "    * 5 pts for at least hypothesis tests\n",
    "    * 5 pts for plotting a decision tree\n",
    "    * 10 pts for evaluating and attempting to improve upon the accuracy of a kNN classifer\n",
    "    * 5 pts for comparing the accuracies of a kNN classifer with a decision tree classifier\n",
    "* 10 pts for fitting a line to Google Trends data\n",
    "* 20 pts for Boston house prices regression\n",
    "    * 5 pts for cleaning the data and for interpretting the correlation matrix heatmap\n",
    "    * 10 pts for evaluating and attempting to improve upon the accuracy of a linear regression model\n",
    "    * 5 pts for comparing the coefficients of determination of a linear regression model with a decision tree regressor\n",
    "* 10 pts for adherence to course [coding standard](https://nbviewer.jupyter.org/github/GonzagaCPSC222/DAs/blob/master/Coding%20Standard.ipynb)\n",
    "    * This includes writing Jupyter Notebooks that tell stories with well-organized, interleaved code cells and markdown cells\n",
    "* 15 pts for quality, clarity, and creativity in the data ethics write-up"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
